    # 위 code 에서 주의할 점!
    # lecture note 에서는 마지막에 Theta grad 에 1/m 을 곱해주는데 d3 에 1/m 을 곱해줘도 상수 계수 이므로 앞으로 전파되어서 문제없다.   
    # 역전파 할 때 뒷 층의 bias 에 대한 가중치는 앞 층으로 전달될 error(delta) 를 계산할 때 활용하지 않는다.
    # 왜냐하면 앞 층으로 전달될 error(delta) 는 앞 층의 가중치들을 갱신하기 위해서 전달하는 것인데 뒷 층의 bias 는 앞 층의 가중치들에
    # 의해 전혀 영향을 받지 않는 고정값 1 이기 때문이다. bias 를 제외한 뒷 층의 모든 a(z 에서 activation function 통과), z 는
    # 앞 층의 가중치로 연산하여 나온 결과의므로 이들과 관련된 가중치들 만을 활용하여 앞 층에 전달할 delta 를 계산한다.
    # deep learning from scratch 에서는 이에 대한 고민이 필요없었다. 왜냐면 bias vector 가 따로 있었기 때문.
    # 여기서는 bias 가 theta_0 로 같이 붙어있다.
    # deep learning from scratch 와 다르게 가중치의 크기 와 row, column 이 조금씩 다르다
    # 따라서 핵심은 가중치와 입력값의 크기와 각각의 gradient 의 크기가 동일하다는 점을 이용하여 행렬곱을 잘 맞추면 된다. 유도리있게 하면 됨!!
    
    # foor loop 을 이용해서 구현하라했지만 처음에만 그렇게 해보고 이해한 후 반드시 벡터화 해서 구현할 것. 효율상 차이가 심하다.
    
    
    # neural net weight 들은 절대로 동일한 값으로 초기화해서는 안 된다. 
    # 만약 동일값으로 초기화한다면 각 neuron 에는 동일한 값들이 적재되고 따라서 가중치들이 전부 동일한 값으로 갱신된다. (직접 간단히 구현해서 실험해보자.)
    # 아주 작은 neural net (활성화 함수, 학습률 생략) 으로 실험해보면 쉽게 알 수 있다. (x = [1, 2, 3] , theta1 = [[2, 2, 2], [2, 2, 2]], theta2 = [2, 2] 라고 하면
    # hidden layer에 [12, 12] 이 적제되고 output layer에 [48] 가 적제된다. backprop 하면 (y = 40) 각각 (h-y) 즉, 8이 역전파 되고 theta2에 [96, 96]씩 동일한 값이 갱신
    # theta1 에는 [16, 16] 가 역전파 되고 theta1에 [16, 32, 64]씩 갱신된다. x 가 여러개 입력되면 theta1 의 grad 는 각 열마다 동일한 값으로 갱신된다.
    # 따라서 전부 동일한 값으로 초기화 하면 모든 각 layer 마다의 가중치가 동일하게 갱신되어 동일한 값이 되므로 layer 을 길고 넓게 하는 의미가 없어짐.
    # initalize_with_same_value.py 에 두개의 x 에 대해서 grad 를 간단히 구해보았다.
