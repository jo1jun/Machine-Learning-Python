    # 위 code 에서 주의할 점!
    # lecture note 에서는 마지막에 Theta grad 에 1/m 을 곱해주는데 d3 에 1/m 을 곱해줘도 상수 계수 이므로 앞으로 전파되어서 문제없다.   
    # 역전파 할 때 뒷 층의 bias 에 대한 가중치는 앞 층으로 전달될 error(delta) 를 계산할 때 활용하지 않는다.
    # 왜냐하면 앞 층으로 전달될 error(delta) 는 앞 층의 가중치들을 갱신하기 위해서 전달하는 것인데 뒷 층의 bias 는 앞 층의 가중치들에
    # 의해 전혀 영향을 받지 않는 고정값 1 이기 때문이다. bias 를 제외한 뒷 층의 모든 a(z 에서 activation function 통과), z 는
    # 앞 층의 가중치로 연산하여 나온 결과의므로 이들과 관련된 가중치들 만을 활용하여 앞 층에 전달할 delta 를 계산한다.
    # deep learning from scratch 에서는 이에 대한 고민이 필요없었다. 왜냐면 bias vector 가 따로 있었기 때문.
    # 여기서는 bias 가 theta_0 로 같이 붙어있다.
    # deep learning from scratch 와 다르게 가중치의 크기 와 row, column 이 조금씩 다르다
    # 따라서 핵심은 가중치와 입력값의 크기와 각각의 gradient 의 크기가 동일하다는 점을 이용하여 행렬곱을 잘 맞추면 된다. 유도리있게 하면 됨!!
    
    # foor loop 을 이용해서 구현하라했지만 처음에만 그렇게 해보고 이해한 후 반드시 벡터화 해서 구현할 것. 효율상 차이가 심하다.
    
    
    # neural net weight 들은 절대로 동일한 값으로 초기화해서는 안 된다. 
    # 만약 동일값으로 초기화한다면 각 neuron 에는 동일한 값들이 적재되고 따라서 가중치들이 전부 동일한 값으로 갱신된다. 
    # 아주 작은 neural net 으로 실험해보면 쉽게 알 수 있다.
